{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    - X (array-like): The input features.\n",
    "    - y (array-like): The target variable.\n",
    "    - test_size (float, optional): The proportion of the data to include in the test set. Default is 0.2.\n",
    "    - random_state (int, optional): The seed value for the random number generator. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    - X_train (array-like): The training set input features.\n",
    "    - X_test (array-like): The test set input features.\n",
    "    - y_train (array-like): The training set target variable.\n",
    "    - y_test (array-like): The test set target variable.\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # Shuffle the indices\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Calculate the number of samples for the test set\n",
    "    test_samples = int(len(X) * test_size)\n",
    "    \n",
    "    # Split the indices into train and test sets\n",
    "    test_indices = indices[:test_samples]\n",
    "    train_indices = indices[test_samples:]\n",
    "    \n",
    "    # Split the data based on the indices and return them\n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_model(training, target, testing):\n",
    "    \"\"\"\n",
    "    Predicts the values of the target variable using an affine model fitted to the training data.\n",
    "\n",
    "    Args:\n",
    "        training (numpy.ndarray): Array of shape (n, m) containing the training data.\n",
    "        target (numpy.ndarray): Array of shape (n, 1) containing the target variable.\n",
    "        testing (numpy.ndarray): Array of shape (k, m) containing the testing data.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of shape (k, 1) containing the predicted values of the target variable.\n",
    "    \"\"\"\n",
    "    # Add a column of ones to the training data\n",
    "    training = np.column_stack((training, np.ones(len(training))))\n",
    "    testing = np.column_stack((testing, np.ones(len(testing))))\n",
    "\n",
    "    # Compute the least squares solution\n",
    "    theta, _, _, _ = np.linalg.lstsq(training, target)\n",
    "\n",
    "    # Predict the values of the target variable\n",
    "    return testing @ theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moore(file_path: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the doubling time (in years) for transistor counts estimated via least squares\n",
    "    for the data points in the given CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path of the CSV file containing the data.\n",
    "\n",
    "    Returns:\n",
    "        float: The doubling time (in years) for transistor counts.\n",
    "\n",
    "    \"\"\"\n",
    "    # Read the data from the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract the data from the DataFrame\n",
    "    A = data['Year'].to_numpy()\n",
    "    \n",
    "    b = data['Transistors'].to_numpy()\n",
    "    # Compute the base 2 logarithm of the transistor counts\n",
    "    b = np.log2(b)\n",
    "\n",
    "    # Fit an affine model to the data\n",
    "    training = np.column_stack((A, np.ones(len(A))))\n",
    "    gamma, _, _, _ = np.linalg.lstsq(training, b)\n",
    "\n",
    "    return 1/gamma[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_validation_error(features, b) -> int:\n",
    "    \"\"\"\n",
    "    Finds the value of k that minimizes the sum of squared residuals on the validation set.\n",
    "\n",
    "    Args:\n",
    "        features (numpy array): Array of shape (n, m) containing the dataset features.\n",
    "        b (numpy array): Array of shape (n,) containing the target values.\n",
    "\n",
    "    Returns:\n",
    "        int: The value of k that minimizes the sum of squared residuals on the validation set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(features, b, test_size=0.5, random_state=42)\n",
    "\n",
    "    min_error = 0\n",
    "    best_k = 0\n",
    "    for k in range(features.shape[1]):\n",
    "        selected_features = X_train[:, :k+1]\n",
    "\n",
    "        # Compute the least squares solution for the training set\n",
    "        theta, _, _, _ = np.linalg.lstsq(selected_features, y_train)\n",
    "\n",
    "        # Compute the sum of squared residuals on the validation set\n",
    "        val_features = X_val[:, :(k+1)]\n",
    "        val_predictions = val_features @ theta # use the model to predict CMEDV values for the validation set\n",
    "        residuals = val_predictions - y_val\n",
    "        error = (residuals**2).sum()\n",
    "\n",
    "        if k == 1 or error < min_error:\n",
    "            min_error = error\n",
    "            best_k = k\n",
    "    \n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "exclude"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1samk\\AppData\\Local\\Temp\\ipykernel_18780\\520671929.py:18: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  theta, _, _, _ = np.linalg.lstsq(training, target)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5.07881158e+09,  2.16712289e+09,  5.26079212e+09,  1.62118126e+09,\n",
       "        1.25722018e+09, -2.20041014e+09, -2.38239068e+09, -2.01842960e+09,\n",
       "        4.35088941e+09,  4.35088941e+09,  4.71485049e+09,  5.26079212e+09,\n",
       "       -1.83644905e+09,  5.26079212e+09,  3.98692832e+09,  5.62475321e+09,\n",
       "        5.26079212e+09,  1.62118126e+09,  5.26079212e+09, -1.66436248e+07,\n",
       "       -5.62585254e+08, -1.29050743e+09,  1.80316180e+09,  3.44098669e+09,\n",
       "        1.98514235e+09,  3.98692832e+09,  3.44098669e+09, -1.65446851e+09,\n",
       "        1.65336918e+08,  2.16712289e+09,  5.26079212e+09,  1.98514235e+09,\n",
       "       -2.56437123e+09,  4.53286995e+09,  3.07702561e+09,  5.26079212e+09,\n",
       "        3.62296723e+09,  5.62475321e+09])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('transistors.csv')\n",
    "\n",
    "A = data['Year'].to_numpy()\n",
    "b = data['Transistors'].to_numpy()\n",
    "\n",
    "# Assuming 'data' is your dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(A, b, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "predict_from_model(X_train, y_train, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COSI165",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
